{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756095dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set correct path for imports for this notebook\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# external imports \n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# local imports\n",
    "from src.fetch.google import get_place_reviews, get_place_reviews_de, get_place_id\n",
    "import src.nlp.dish_extractor as dish_extractor\n",
    "from src.normalise.schema import REVIEW_SCHEMA, REVIEW_MAPS\n",
    "from src.normalise.normaliser import normalise_review\n",
    "from src.nlp.dish_extractor import conv_to_food_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b2441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RESTAURANT DETAILS ---\n",
      "FunkyFisch\n",
      "Kantstraße 135-136, 10625 Berlin, Deutschland\n",
      "ChIJU7-KvONQqEcRCmPCCvo-Fg4\n",
      "--- END ---\n",
      " \n",
      "--- RESTAURANT DETAILS ---\n",
      "Standard Serious Pizza Charlottenburg\n",
      "Schlüterstraße 63, 10625 Berlin, Deutschland\n",
      "ChIJc2uwDI1RqEcRfAsy3YxyfIY\n",
      "--- END ---\n",
      " \n",
      "--- RESTAURANT DETAILS ---\n",
      "Borchardt\n",
      "Französische Str. 47, 10117 Berlin, Deutschland\n",
      "ChIJVfEmINtRqEcRsch0pslaanQ\n",
      "--- END ---\n",
      " \n"
     ]
    }
   ],
   "source": [
    "r1_id = get_place_id(\"funkyfish berlin\")\n",
    "r2_id = get_place_id(\"standard pizza west berlin\")\n",
    "r3_id = get_place_id(\"borschert berlin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f7156c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google reviews retrieved, found 5 reviews for FunkyFisch\n",
      " \n",
      "✅ Google reviews retrieved, found 5 reviews for Standard Pizza West\n",
      " \n",
      "✅ Google reviews retrieved, found 5 reviews for Borchardt\n",
      " \n"
     ]
    }
   ],
   "source": [
    "r1_reviews = get_place_reviews(r1_id)\n",
    "r2_reviews = get_place_reviews(r2_id)\n",
    "r3_reviews = get_place_reviews(r3_id)\n",
    "r1_de_reviews = get_place_reviews_de(r1_id)\n",
    "r3_de_reviews = get_place_reviews_de(r3_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b03e85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r1_de_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55af999d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model cardiffnlp/twitter-roberta-base-sentiment-latest with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>). See the original errors:\n\nwhile loading with AutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5048, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5316, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 508, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1647, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5048, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5316, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 508, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1647, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nwhile loading with RobertaForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5048, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5316, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 508, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1647, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5048, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5316, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 508, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1647, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sentiment = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentiment-analysis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcardiffnlp/twitter-roberta-base-sentiment-latest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m r2_reviews:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(sentiment(review[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1027\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:333\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    331\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback.items():\n\u001b[32m    332\u001b[39m             error += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    334\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    335\u001b[39m         )\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    338\u001b[39m     framework = infer_framework(model.\u001b[34m__class__\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not load model cardiffnlp/twitter-roberta-base-sentiment-latest with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>). See the original errors:\n\nwhile loading with AutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5048, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5316, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 508, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1647, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5048, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5316, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 508, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1647, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nwhile loading with RobertaForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5048, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5316, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 508, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1647, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5048, in from_pretrained\n    ) = cls._load_pretrained_model(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 5316, in _load_pretrained_model\n    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 508, in load_state_dict\n    check_torch_load_is_safe()\n  File \"c:\\Users\\robin\\Desktop\\Bootcamp_Files\\capstone-dishtip-be\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1647, in check_torch_load_is_safe\n    raise ValueError(\nValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0581a9849cec4e21a671c3b76625d32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "for review in r2_reviews:\n",
    "    print(sentiment(review[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9dd9da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'FOOD',\n",
       "  'score': 0.79940397,\n",
       "  'word': 'soup',\n",
       "  'start': 182,\n",
       "  'end': 186},\n",
       " {'entity_group': 'FOOD',\n",
       "  'score': 0.9811435,\n",
       "  'word': 'o',\n",
       "  'start': 273,\n",
       "  'end': 274},\n",
       " {'entity_group': 'FOOD',\n",
       "  'score': 0.9928268,\n",
       "  'word': '##yster',\n",
       "  'start': 274,\n",
       "  'end': 279},\n",
       " {'entity_group': 'FOOD',\n",
       "  'score': 0.92414606,\n",
       "  'word': '##s',\n",
       "  'start': 279,\n",
       "  'end': 280},\n",
       " {'entity_group': 'FOOD',\n",
       "  'score': 0.98772764,\n",
       "  'word': 'o',\n",
       "  'start': 361,\n",
       "  'end': 362},\n",
       " {'entity_group': 'FOOD',\n",
       "  'score': 0.99351007,\n",
       "  'word': '##yster',\n",
       "  'start': 362,\n",
       "  'end': 367},\n",
       " {'entity_group': 'FOOD',\n",
       "  'score': 0.9716562,\n",
       "  'word': '##s',\n",
       "  'start': 367,\n",
       "  'end': 368}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "ner = pipeline(\"ner\", model=\"Dizex/FoodBaseBERT-NER\", aggregation_strategy=\"simple\")\n",
    "\n",
    "food1 = ner(r1_reviews[1][\"text\"], aggregation_strategy=\"simple\")\n",
    "food1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4745af13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Restaurants',\n",
       " 'Lachs',\n",
       " 'Oktopus-Carpaccio',\n",
       " 'Tuna',\n",
       " 'Risotto',\n",
       " 'Funky Pasta',\n",
       " 'Riesengarnel',\n",
       " 'T',\n",
       " 'fi',\n",
       " 'steak',\n",
       " 'biert',\n",
       " 'Z',\n",
       " 'Funky']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Dizex/FoodBaseBERT\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Dizex/FoodBaseBERT\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"\"\"Eines der besten Restaurants, die ich je besucht habe! Wir hatten als Vorspeise Lachs- und Oktopus-Carpaccio – beides unglaublich frisch und köstlich. Als Hauptgerichte haben wir den Tuna mit Risotto, Funky Pasta, Riesengarnelen und das Thunfischsteak probiert. Jedes Gericht war perfekt zubereitet und ein echtes Geschmackserlebnis. Dazu hatten wir einen hervorragenden Wein, der das Ganze wunderbar abgerundet hat.\n",
    "\n",
    "Die Location ist modern und stilvoll, das Personal super freundlich und aufmerksam. Man merkt, dass hier großer Wert auf Qualität gelegt wird,sowohl bei den Zutaten als auch beim Service.\n",
    "\n",
    "Für mich ist FunkyFish ein Highlight in Berlin und eine klare Empfehlung für alle, die frisches und exzellentes Essen lieben. Vielen Dank für diesen unvergesslichen Abend!\"\"\"\n",
    "\n",
    "ner_entity_results = pipe(example)\n",
    "conv_to_food_string(example, ner_entity_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb1b52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'YES'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "extractor = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "\n",
    "r = r3_de_reviews[4][\"text\"]\n",
    "\n",
    "prompt = (\n",
    "    \"Identify all food or dish names in the text below that are positively mentioned by the reviewer.\\n\"\n",
    "    \"If no dishes are positively mentioned, respond with 'none'.\\n\" \n",
    "    \"Respond only with a comma-separated list of dish names.\\n\\n\"\n",
    "    f\"Input: {r}\\nOutput:\"\n",
    ")\n",
    "\n",
    "prompt_de = (\n",
    "    \"Erkenne alle Gerichte oder Speisen im folgenden Text, die vom Bewerter positiv erwähnt werden.\\n\"\n",
    "    \"Wenn keine Gerichte positiv erwähnt werden, antworte mit 'none'.\\n\"\n",
    "    \"Antworte nur mit einer durch Kommas getrennten Liste der Gerichte.\\n\\n\"\n",
    "    f\"Text: {r}\\nAntwort:\"\n",
    ")\n",
    "\n",
    "\n",
    "result = extractor(prompt_de, max_new_tokens=50)\n",
    "print(result)\n",
    "\n",
    "\n",
    "# Use after \"respond with none\" if results get shit.\n",
    "    # \"Example:\\n\"\n",
    "    # \"Input: I loved the sushi, but the miso soup was bad.\\n\"\n",
    "    # \"Output: sushi\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abaa914c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Essen und Service sind absolut top! Das Essen ist herausragend - vor allem das Schnitzel!\\n\\nAnbieten ist etwas „altbacken“, aber dennoch schön.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3_de_reviews[4][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fde882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'FunkyFish'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "extractor = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "text = \"I loved the Margherita pizza and tiramisu, but the lasagna was too salty.\"\n",
    "\n",
    "prompt = (\n",
    "    \"Identify all food or dish names in the text below that are positively mentioned by the reviewer. \"\n",
    "    \"Respond only with a comma-separated list of dish names.\\n\\n\"\n",
    "    \"Example:\\n\"\n",
    "    \"Input: I loved the sushi, but the miso soup was bad.\\n\"\n",
    "    \"Output: sushi\\n\\n\"\n",
    "    f\"Input: {example}\\nOutput:\"\n",
    ")\n",
    "\n",
    "result = extractor(prompt, max_new_tokens=100)\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c5ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1456fa5cbbd54f3a9e5e3daec1797efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5563ff9d32894f748d6ed289ba3b1479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_deepseek.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct:\n",
      "- configuration_deepseek.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217a81c4e55f43b68fbb208953e17f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_deepseek.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct:\n",
      "- modeling_deepseek.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c90e45c8a77422eb7dde86676cd12ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38217d9035eb4c4a9558a369fcaee74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cd6b6d7d19423f96c1b6b03a5c13a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-000004.safetensors:   0%|          | 0.00/5.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c5afc3d85547a38875051cfcabb683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000004.safetensors:   0%|          | 0.00/8.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c07c40ae2c467e9a49b18484b59197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-000004.safetensors:   0%|          | 0.00/8.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321920b7f24247d3ad3365ed01ee7bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-000004.safetensors:   0%|          | 0.00/8.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\", trust_remote_code=True)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
